# -*- coding: utf-8 -*-
"""credit_card_fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mVlZdFxCDxiMRj7_SCo5EvVkk_c4WRen

Importing library and dataset
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
import warnings
warnings.filterwarnings("ignore")

dataset = pd.read_csv('creditcard.csv')

X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

"""Exploratory Data Analysis."""

dataset.info()

dataset.head()

dataset.describe()

"""Checking the dataset for nulls"""

dataset.isnull().sum()

"""Boxplots of Class with Amounts"""

columns_to_plot = ['Amount']
dataset[columns_to_plot].boxplot()

plt.title('Boxplot of Amount and Class')
plt.ylabel('Values')
plt.show()

"""The dataset contains outliers. Dropping the outliers. Dropping the values which are above Q3 and below Q1 in Amount column."""

df= dataset[(dataset['Amount'] <= 10000)]

columns_to_plot = ['Amount']
df[columns_to_plot].boxplot()

plt.title('Boxplot of Amount and Class')
plt.ylabel('Values')
plt.show()

correlation_matrix = df.corr()

plt.figure(figsize=(10, 8))
heatmap = sns.heatmap(correlation_matrix, cmap="coolwarm")

plt.title('Correlation Between Columns')

cax = plt.gcf().axes[-1]
cax.set_ylabel('Correlation')

plt.show()

class_counts = df['Class'].value_counts()

print("Number of values in Class column:")
print(class_counts)

correlation_values = correlation_matrix.abs().unstack().sort_values(ascending=False)

# Filter out self-correlations (where both columns are the same)
correlation_values = correlation_values[correlation_values != 1]

# Select the highest correlation pairs
highest_correlation_pairs = correlation_values.head()

print("Columns with the highest correlation:")
for pair, correlation in highest_correlation_pairs.items():
    column1, column2 = pair
    print(f"{column1} and {column2}: {correlation}")

"""From this we can see that features V2 and V7 have the highest correlation with Amount and V3 has the highest correlation with Time."""

plt.scatter(df['V2'], df['V7'])
plt.xlabel('V2')
plt.ylabel('V7')
plt.title('Scatter Plot of V2 and V7')
plt.show()

plt.scatter(df['V2'], df['Amount'])
plt.xlabel('V2')
plt.ylabel('Amount')
plt.title('Scatter Plot of V2 and Amount')
plt.show()

plt.scatter(df['V7'], df['Amount'])
plt.xlabel('V7')
plt.ylabel('Amount')
plt.title('Scatter Plot of V7 and Amount')
plt.show()

columns_to_plot = ['V2', 'V7', 'Amount']

plt.figure(figsize=(10, 6))

# Scatter plot of 'V2' vs 'V7'
plt.scatter(df['V2'], df['V7'], label='V2 vs V7', alpha=0.5)

# Scatter plot of 'V2' vs 'Amount'
plt.scatter(df['V2'], df['Amount'], label='V2 vs Amount', alpha=0.5)

# Scatter plot of 'V7' vs 'Amount'
plt.scatter(df['V7'], df['Amount'], label='V7 vs Amount', alpha=0.5)

plt.xlabel('V2')
plt.ylabel('V7 / Amount')
plt.title('Scatter Plot of V2, V7, and Amount')
plt.legend()
plt.show()

plt.scatter(df['V3'], df['Time'])
plt.xlabel('V3')
plt.ylabel('Time')
plt.title('Scatter Plot of V3 and Time')
plt.show()

class_1_data = df[df['Class'] == 1]

columns_to_plot = ['V2']

plt.figure(figsize=(12, 6))

for i, column in enumerate(columns_to_plot):
    plt.subplot(1, len(columns_to_plot), i + 1)
    plt.hist(class_1_data['V2'], bins=20)
    plt.xlabel('V2')
    plt.ylabel('Fraud Counts')

plt.tight_layout()
plt.show()

columns_to_plot = ['V3']

plt.figure(figsize=(12, 6))

for i, column in enumerate(columns_to_plot):
    plt.subplot(1, len(columns_to_plot), i + 1)
    plt.hist(class_1_data['V3'], bins=20)
    plt.xlabel('V3')
    plt.ylabel('Fraud Counts')

plt.tight_layout()
plt.show()

columns_to_plot = ['V7']

plt.figure(figsize=(12, 6))

for i, column in enumerate(columns_to_plot):
    plt.subplot(1, len(columns_to_plot), i + 1)
    plt.hist(class_1_data['V7'], bins=20)
    plt.xlabel('V7')
    plt.ylabel('Fraud Counts')

plt.tight_layout()
plt.show()

"""Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

"""Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""Training the Logistic Regression model on the Training set"""

from scipy.sparse import random
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(X_train, y_train)

"""Predicting the Test set results"""

y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

"""Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)

accuracy_score(y_test, y_pred)

from sklearn.metrics import precision_recall_curve, auc

features = df.drop('Class', axis=1)
target = df['Class']

plt.figure(figsize=(10, 6))

for feature in features.columns:
    precision, recall, _ = precision_recall_curve(target, features[feature])
    auprc = auc(recall, precision)
    plt.plot(recall, precision, label=f'{feature}')

    last_recall = recall[-1]
    last_precision = precision[-1]
    plt.annotate(f'AUPRC={auprc:.2f}', xy=(last_recall, last_precision), xytext=(last_recall + 0.05, last_precision))

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Features')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()