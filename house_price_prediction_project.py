# -*- coding: utf-8 -*-
"""House Price Prediction Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OrsR531AsJIHxPjLuFIvUYBe8vmbqAC3

**Problem statement: **

---


RAJ Property Brokers Inc. is a new company in Real Estate segment of Seattle. They help people in finding their dream homes. For this purpose they want to learn about what different factors affect the price of properties in the area of Seattle and it's surrounding areas to help their clients be better able to budget and bargain on the prices. They also want to know how much a factor affects the price and which factores are the strongest determinants of price. We collect the data of different house transaction with various features for the period 2014 and 2015 and begin our analysis.

**We shall start by installing the required libraries and importing the required libraries.**
"""

pip install catboost

pip install xgboost

pip install folium

"""Here, we begin by importing all the necessary libraries we shall be using."""

# Importing the libraries needed for Data Cleaning and Pre Processing

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")
from datetime import datetime, timedelta

# Importing the libraries needed for Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Importing the libraries needed for mapping of the geographical data

import folium
from folium.plugins import FastMarkerCluster
sns.set()
sns.set_palette(palette='deep')

# Importing the libraries needed for model building and hyper tuning the model

from math import sin, cos, sqrt, atan2, radians
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from catboost import CatBoostRegressor
from sklearn import svm
from sklearn.svm import SVC
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.datasets import make_regression
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import BayesianRidge
from sklearn.tree import DecisionTreeRegressor
import statsmodels.api as sm
from sklearn.feature_selection import RFE
from catboost import CatBoostRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.impute import KNNImputer
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy import stats
from scipy.stats import ks_2samp, norm

"""Importing the data in a dataframe named 'houses'"""

houses = pd.read_excel('/content/innercity (1).xlsx')

"""We'll start by cheking the shape of the dataset and the data types of the columns present in the data."""

houses.info()

houses.shape

pd.set_option('display.max_columns', None)

print(houses.head())

"""**Insights:**
 * Here, we can see that the data has 23 columns and 21613 rows along with null values in various columns.
 * For the imputation of null values, we will check the description of the data and unique values present in the data.
"""

houses.describe()

"""**Insights:**
* Out of 23 columns, 16 have been taken as numerical
columns while the rest of the 7 coulmns have been taken as objects.
* This is incorrect classification since they contain mostly numerical data.
* So we shall extract unique values in all columns to inspect the reason behind the misclassification.
"""

for column in houses.columns:
    unique_values = houses[column].unique()
    print(f'Column "{column}" has unique values: {unique_values}')

"""**Insights:**
* We observed that '$' symbol is present in various columns, which can be understood as noise in the data, hence, we shall replace then to null.
* We can also see that yr_renovated has value of '0' which we'll replace with null values for proper analysis.   
"""

houses['cid'] = houses['cid'].replace('$', np.nan)
houses['dayhours'] = houses['dayhours'].replace('$', np.nan)
houses['price'] = houses['price'].replace('$', np.nan)
houses['room_bed'] = houses['room_bed'].replace('$', np.nan)
houses['room_bath'] = houses['room_bath'].replace('$', np.nan)
houses['lot_measure'] = houses['lot_measure'].replace('$', np.nan)
houses['living_measure'] = houses['living_measure'].replace('$', np.nan)
houses['ceil'] = houses['ceil'].replace('$', np.nan)
houses['coast'] = houses['coast'].replace('$', np.nan)
houses['sight'] = houses['sight'].replace('$', np.nan)
houses['condition'] = houses['condition'].replace('$', np.nan)
houses['quality'] = houses['quality'].replace('$', np.nan)
houses['ceil_measure'] = houses['ceil_measure'].replace('$', np.nan)
houses['basement'] = houses['basement'].replace('$', np.nan)
houses['yr_built'] = houses['yr_built'].replace('$', np.nan)
houses['yr_renovated'] = houses['yr_renovated'].replace('$', np.nan)
houses['zipcode'] = houses['zipcode'].replace('$', np.nan)
houses['lat'] = houses['lat'].replace('$', np.nan)
houses['long'] = houses['long'].replace('$', np.nan)
houses['living_measure15'] = houses['living_measure15'].replace('$', np.nan)
houses['lot_measure15'] = houses['lot_measure15'].replace('$', np.nan)
houses['furnished'] = houses['furnished'].replace('$', np.nan)
houses['total_area'] = houses['total_area'].replace('$', np.nan)

houses['yr_renovated'] = houses['yr_renovated'].replace(0, np.nan)

"""
---
Now we'll start by checking the null values for each column and start with the imputation of null values.
"""

null_counts = houses.isnull().sum()

print("Null Values in Each Column:")
print(null_counts)

houses.describe()

"""**Insights:**
* We learnt that **'Yr_renovated' column has 95.77% null values**. Hence we shall not be imputing the null values there but instead **we'll be dropping it**.
* Other columns that seem to be **normally distributed** based on mean and median  are **Ceil and Coast**. As they are Categorical variables, median is giving a proper rounded value to those and we shall **impute them with median**.
* Other columns don't seem to be normally distributed and we shall impute them with either **median or mode** taking a look at the data.  
* **Total Area** is nothing but the **sum of Living_measure and Lot_measure** and we shall **drop the column** along with ZIP-Code as it doesn't help with the Regression analysis.
"""

#imputing Coast
houses['coast'] = houses['coast'].replace(np.nan,houses.coast.median())
#imputing Ceil
houses['ceil'] = houses['ceil'].replace(np.nan,houses.ceil.median())
#imputing yr_built
houses['yr_built'] = houses['yr_built'].replace(np.nan,houses.yr_built.median())

#imputing room_bed
houses.room_bed.mode()
houses.room_bed.median()

houses['room_bed'] = houses['room_bed'].replace(np.nan,houses.room_bed.median())

#imputing room_bath
houses['room_bath'] = houses['room_bath'].replace(np.nan,houses.room_bath.median())

#imputing sight
houses['sight'].value_counts().sort_index()
houses['sight']=houses['sight'].replace(np.nan,0)

#imputing condition
houses['condition'].value_counts().sort_index()
houses['condition'] = houses['condition'].replace(np.nan,3)

#imputing quality
houses['quality'].value_counts().sort_index()
houses['quality'] = houses['quality'].replace(np.nan,7)

#imputing ceil_measure
houses['ceil_measure'] = houses['ceil_measure'].replace(np.nan,houses.ceil_measure.median())

#imputing basement_measure
houses['basement'] = houses['basement'].replace(np.nan,0)

#imputing longitude
houses['long'].fillna(method='ffill', inplace=True)

houses['long'] = houses['long'].astype(float)

#imputing living_measure15
houses['living_measure15'].fillna(houses['living_measure'], inplace=True)

#imputing lot_measure15
houses['lot_measure15'].fillna(houses['lot_measure'], inplace=True)

#imputing furnished
houses['furnished'] = houses['furnished'].replace(np.nan, houses.furnished.median())

"""**Insights:**
* As **Coast, Ceil, Room_bed and Room_bath** were pretty close to normal distribution, we **imputed these using median**.
* We **imputed Yr_built using median** assuming houses were built in the middle period.
* We also **imputed furnished, and ceil_measure using median** as they seemed most suited with the application of median.
* For **sight, condition and quality**, we imputed the values using **unique value counts and selecting the most occuring value i.e. mode**.
* For **longitude, we used forward fill method** to impute the null values to get nearest longitude values.
* For **Living_measure15 and Lot_measure15**, we assumed that the houses were not renovated and hence **imputed them using the old living_measure and lot_measure**. We also decided to **drop the columns living_measure, lot_measure and total area** given that we now have **updated** living_measure and lot_measure.
"""

houses.info()

"""---

**Now we'll start with Feature Engineering to extract the needed data from required columns.**

---
We'll be extracting the **year of sale** of the property from the **Dayhours column**.


---

For this, firstly we shall extract the value of 'Years' from the Dayhours column.
"""

houses['dayhours'] = pd.to_datetime(houses['dayhours'])

houses['year_sold'] = houses['dayhours'].dt.year

houses.year_sold.unique()

houses['age_at_sale'] = houses['year_sold'] - houses['yr_built']

houses['age_at_sale'].unique()

houses['age_at_sale'] = houses['age_at_sale'].replace(-1, 0)

houses['age_at_sale'].unique()

"""**Insights:**
* When we calculated the age at sale, we found a **value of '-1'** which implied that the house was sold before it was built. Here we took the assumption that the **house was sold as soon as the house was built** and took the value as 0.

Next we added a new column which indicates whether a property has been renovated or not.
"""

houses['renovated'] = houses.apply(lambda row: 0 if row['living_measure'] == row['living_measure15'] and row['lot_measure'] == row['lot_measure15'] else 1, axis=1)

"""Next from the 'cid' column, which indicates the Customer ID, we extracted how many times a house has been sold."""

houses['cid'] = houses['cid'].astype('object')

houses['TimesSold'] = houses['cid'].apply(lambda x: (houses['cid'] == x).sum())

houses.TimesSold.unique()

"""

---
We shall map the latitudes and longitudes present in the data to see which state or city has the highest number of properties. After that we will calculate the distance of the properties from 6 main places and tourist attractions.
"""

lat = houses['lat'].tolist()
lon = houses['long'].tolist()
locations = list(zip(lat, lon))

# Most properties are located near Seattle, so we'll taking Seattle's latitude and longitude

map1 = folium.Map(location=[47.608013, -122.335167], zoom_start=12)
FastMarkerCluster(locations).add_to(map1)
map1

"""Top 6 Locations of Seattle:
>King Street Station
>>Museum of Pop Culture (MOPOP)
>>>Colman Dock ferry
>>>>Seattle Japanese Garden
>>>>>T-Mobile Park
>>>>>>Seattle-Tacoma International Airport

Next we'll add 6 new columns in our dataset which will have the distance of all the properties from the top 6 locations. We shall do this by defining a function for distance.
"""

def distance(lat1, lat2, lon1, lon2):
    R = 6373.0 ## Approx radius of earth in KM
    rlat1 = radians(lat1)
    rlat2 = radians(lat2)
    rlon1 = radians(lon1)
    rlon2 = radians(lon2)
    rdlon = rlon2 - rlon1
    rdlat = rlat2 - rlat1
    a = sin(rdlat / 2)**2 + cos(rlat1) * cos(rlat2) * sin(rdlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    distance = R * c
    return distance

# Top locations in Seattle

toploc = {"station": [47.5984, 122.3299],
          "airport": [47.4464, 122.2993],
          "mopop": [47.6215, 122.3481],
          "ferry": [47.6028, 122.3380],
          "jgarden": [47.6290, 122.2963],
          "tmp": [47.5913, 122.3325]}

toploc = pd.DataFrame.from_dict(toploc)
toploc_trans = toploc.transpose()
toploc_trans.columns = ["latitude", "longitude"]

# Constructing distance columns
dist = []
for col in toploc.columns:
    houses["dist_"+col] = houses.apply(lambda x: distance(x.lat, toploc[col][0], x.long, toploc[col][1]), axis=1)
    dist.append("dist_"+col)

"""
---
Next we shall check if a property has a basement or not.
"""

houses['basement_present'] = houses.apply(lambda row: 0 if row['basement'] == 0 else 1, axis=1)

"""
---
Since, price is log-normally distributed variable, we take the log of price and check the plot to test if our information is correct."""

houses['log_price'] = np.log(houses['price']).copy()

sns.histplot(houses['price'], kde=True, color='blue')
plt.xlabel('Price')
plt.ylabel('Density')
plt.title('Density Plot of Price')
plt.show()

sns.histplot(houses['log_price'], kde=True, color='blue')
plt.xlabel('Log Price')
plt.ylabel('Density')
plt.title('Density Plot of Log Price')
plt.show()

"""**Insights:**
* As it can be seen from both the diagrams, log of price follows a normal distribution while price is highly positively skewed.
* Here we shall keep the log price and drop the variable price.
"""

houses.info()

"""**Insights:**
* Here we drop columns CID, living_measure, lot_measure, dayhours, price,yr_renovated, ZIP code, lat, long and total area.
* CID simply means Identification number.
* Living_measure and lot_measure will be dropped as living_measure15 and lot_measure15 are their updated data.
* Dayhours will be dropped because we have extracted year sold and age at sale from it.
* Price will be dropped because we have scaled it using it's log.
* yr_renovated will be dropped as it has 95.77% null values.
* ZIP code, lat and long will be dropped because we have taken distance from 6 most popular destinations.
* Total area will be dropped because it is a sum of the living_measure and lot_measure and both these columns contain old data.
"""

houses.drop(['cid','living_measure','lot_measure','dayhours','price','yr_renovated','zipcode','lat','long','total_area'],axis=1, inplace=True)

houses.info()

"""Here, we re-order the data with log of price as first columns as it becomes easy for us to use factor importance and further analysis."""

houses_reordered = houses[['log_price','room_bed','room_bath','ceil','coast','sight','condition','quality','ceil_measure','basement','yr_built','living_measure15','lot_measure15'
                          ,'furnished','year_sold','age_at_sale','renovated','TimesSold','dist_station','dist_airport','dist_mopop','dist_ferry','dist_jgarden','dist_tmp'
                          ,'basement_present']].copy()

houses_reordered.info()

"""
---

Now, we check if our data has any duplicate data present"""

num_duplicate_rows = houses_reordered.duplicated().sum()
print("Number of Duplicate Rows:", num_duplicate_rows)

duplicate_mask = houses_reordered.duplicated(keep=False)

duplicate_rows = houses_reordered[duplicate_mask]
duplicate_rows.sort_values(by=duplicate_rows.columns.tolist(), inplace=True)

pd.options.display.width = 0
print("All Rows Pertaining to Duplicate Rows:")
print(duplicate_rows.to_string(line_width=10000))

excel_file = 'Duplicate_rows.xlsx'

duplicate_rows.to_excel(excel_file, index=True)

"""**Insights:**
* Our data indeed consists of duplicates. Now we drop the duplicates and move further analyzing the data.
"""

houses = houses_reordered.drop_duplicates()

print(houses)

houses.info()

"""Now we'll do an **Univariate analysis** for the data. For this we shall use **Boxplots.** Now we'll run boxplot for univariate analysis of each variable."""

houses.describe()

for column in houses.columns:
    plt.figure(figsize=(6, 4))
    houses.boxplot(column=[column], vert=True)
    plt.title(f"Boxplot of {column}")
    plt.show()

"""**Insights:**
* Based on our Univariate analysis, we can see that **there are outliers** in columns log_price, room_bed, room_bath, living_measure15, lot_measure15, ceil_measure, basement and distance columns that we decided to **cap using IQR**.
* Columns like coast and furnished are binary variables and hence we decided to not cap the outliers.
* Columns like sight, condition, quality, we were able to spot outliers and decide to check and analyze them further checking.
"""

houses['sight'].value_counts().sort_index()

houses['condition'].value_counts().sort_index()

houses['quality'].value_counts().sort_index()

"""**Insights:**
* Here, we find that sight, quality and condition are categorical variables and we shall not be treating them for outliers.

Now we shall begin to do our **Bivariate analysis** using a **Correlation matrix** and presenting it using a **Heatmap**.
"""

correlation_matrix = houses.corr()

plt.figure(figsize=(13, 8))
sns.heatmap(correlation_matrix, annot=True,annot_kws={'size': 12}, cmap='coolwarm', linewidths=1)
plt.title('Correlation Matrix Heatmap')
plt.show()

"""**Insights:**
* Given that we were not able to see the heat map properly, we move to extract the correlation matrix for further analysis.
"""

correlation_matrix

correlation_matrix_df = pd.DataFrame(correlation_matrix)

excel_file = 'Correlation_Matrix.xlsx'

correlation_matrix_df.to_excel(excel_file, index=True)

"""**Insights:**
* Distance columns are perfectly positively correlated with each other
* Basement and Basement Present has correlation of 0.8, showing a strong positive correlation.
* Year built and Age at sale have a correlation of -1, showing highly negative correlation.
* 5 features have a correlation in the range of 0.7.

We also decided to run a **Pair Plot** for better analysis.
"""

num_col = houses.select_dtypes(exclude=['object']).columns
ax = sns.pairplot(houses[num_col])

"""**Insights:**
> In the Bivariate Analysis, we find that there is a high correlation between variables.

We begin by scaling the numerical value columns living_measure15, lot_measure15, ceil_measure and basement using Min Max Scaler to better analyze the data.
"""

scaler = MinMaxScaler()

houses['scaled_living_measure15'] = scaler.fit_transform(houses[['living_measure15']])
houses['scaled_basement'] = scaler.fit_transform(houses[['basement']])
houses['scaled_ceil_measure'] = scaler.fit_transform(houses[['ceil_measure']])
houses['scaled_lot_measure15'] = scaler.fit_transform(houses[['lot_measure15']])

"""Now we shall drop the original columns of the scaled data columns."""

houses.drop(['living_measure15','basement','ceil_measure','lot_measure15'],axis=1, inplace=True)

houses.info()

"""Now we'll check which columns have outliers and proceed accordingly."""

def count_outliers_iqr(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = (data < lower_bound) | (data > upper_bound)
    num_outliers = outliers.sum()

    return num_outliers


outliers_count_per_column = houses.apply(count_outliers_iqr)

print("Count of outliers in each column:")
print(outliers_count_per_column)

"""**Insights:**
* As we can see, columns log_price, room_bed, room_bath, living_measure15, lot_measure15, ceil_measure, basement, dist_station, dist_airport, dist_mopop, dist_ferry, dist_jgarden and dist_tmp have outliers. So now we'll begin to cap these columns using **IQR**
"""

LPQ1 = houses['log_price'].quantile(0.25)
LPQ3 = houses['log_price'].quantile(0.75)
LP_IQR = LPQ3 - LPQ1

LP_lower_bound = LPQ1 - 1.5 * LP_IQR
LP_upper_bound = LPQ3 + 1.5 * LP_IQR

LP_outliers_count = len(houses[(houses['log_price'] < LP_lower_bound) | (houses['log_price'] > LP_upper_bound)])

print("Count of outliers in 'log_price' column:", LP_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['log_price'] = np.where(houses['log_price'] < LP_lower_bound, LP_lower_bound, houses['log_price'])
houses['log_price'] = np.where(houses['log_price'] > LP_upper_bound, LP_upper_bound, houses['log_price'])

RbedQ1 = houses['room_bed'].quantile(0.25)
RbedQ3 = houses['room_bed'].quantile(0.75)
Rbed_IQR = RbedQ3 - RbedQ1

Rbed_lower_bound = RbedQ1 - 1.5 * Rbed_IQR
Rbed_upper_bound = RbedQ3 + 1.5 * Rbed_IQR

Rbed_outliers_count = len(houses[(houses['room_bed'] < Rbed_lower_bound) | (houses['room_bed'] > Rbed_upper_bound)])

print("Count of outliers in 'room_bed' column:", Rbed_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['room_bed'] = np.where(houses['room_bed'] < Rbed_lower_bound, Rbed_lower_bound, houses['room_bed'])
houses['room_bed'] = np.where(houses['room_bed'] > Rbed_upper_bound, Rbed_upper_bound, houses['room_bed'])

RbathQ1 = houses['room_bath'].quantile(0.25)
RbathQ3 = houses['room_bath'].quantile(0.75)
Rbath_IQR = RbathQ3 - RbathQ1

Rbath_lower_bound = RbathQ1 - 1.5 * Rbath_IQR
Rbath_upper_bound = RbathQ3 + 1.5 * Rbath_IQR

Rbath_outliers_count = len(houses[(houses['room_bath'] < Rbath_lower_bound) | (houses['room_bath'] > Rbath_upper_bound)])

print("Count of outliers in 'room_bath' column:", Rbath_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['room_bath'] = np.where(houses['room_bath'] < Rbath_lower_bound, Rbath_lower_bound, houses['room_bath'])
houses['room_bath'] = np.where(houses['room_bath'] > Rbath_upper_bound, Rbath_upper_bound, houses['room_bath'])

SBQ1 = houses['scaled_basement'].quantile(0.25)
SBQ3 = houses['scaled_basement'].quantile(0.75)
SB_IQR = SBQ3 - SBQ1

SB_lower_bound = SBQ1 - 1.5 * SB_IQR
SB_upper_bound = SBQ3 + 1.5 * SB_IQR

SB_outliers_count = len(houses[(houses['scaled_basement'] < SB_lower_bound) | (houses['scaled_basement'] > SB_upper_bound)])

print("Count of outliers in 'scaled_basement' column:", SB_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['scaled_basement'] = np.where(houses['scaled_basement'] < SB_lower_bound, SB_lower_bound, houses['scaled_basement'])
houses['scaled_basement'] = np.where(houses['scaled_basement'] > SB_upper_bound, SB_upper_bound, houses['scaled_basement'])

SCMQ1 = houses['scaled_ceil_measure'].quantile(0.25)
SCMQ3 = houses['scaled_ceil_measure'].quantile(0.75)
SCM_IQR = SCMQ3 - SCMQ1

SCM_lower_bound = SCMQ1 - 1.5 * SCM_IQR
SCM_upper_bound = SCMQ3 + 1.5 * SCM_IQR

SCM_outliers_count = len(houses[(houses['scaled_ceil_measure'] < SCM_lower_bound) | (houses['scaled_ceil_measure'] > SCM_upper_bound)])

print("Count of outliers in 'scaled_ceil_measure' column:", SCM_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['scaled_ceil_measure'] = np.where(houses['scaled_ceil_measure'] < SCM_lower_bound, SCM_lower_bound, houses['scaled_ceil_measure'])
houses['scaled_ceil_measure'] = np.where(houses['scaled_ceil_measure'] > SCM_upper_bound, SCM_upper_bound, houses['scaled_ceil_measure'])

SLMQ1 = houses['scaled_living_measure15'].quantile(0.25)
SLMQ3 = houses['scaled_living_measure15'].quantile(0.75)
SLM_IQR = SLMQ3 - SLMQ1

SLM_lower_bound = SLMQ1 - 1.5 * SLM_IQR
SLM_upper_bound = SLMQ3 + 1.5 * SLM_IQR

SLM_outliers_count = len(houses[(houses['scaled_living_measure15'] < SLM_lower_bound) | (houses['scaled_living_measure15'] > SLM_upper_bound)])

print("Count of outliers in 'scaled_living_measure15' column:", SLM_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['scaled_living_measure15'] = np.where(houses['scaled_living_measure15'] < SLM_lower_bound, SLM_lower_bound, houses['scaled_living_measure15'])
houses['scaled_living_measure15'] = np.where(houses['scaled_living_measure15'] > SLM_upper_bound, SLM_upper_bound, houses['scaled_living_measure15'])

SLOMQ1 = houses['scaled_lot_measure15'].quantile(0.25)
SLOMQ3 = houses['scaled_lot_measure15'].quantile(0.75)
SLOM_IQR = SLOMQ3 - SLOMQ1

SLOM_lower_bound = SLOMQ1 - 1.5 * SLOM_IQR
SLOM_upper_bound = SLOMQ3 + 1.5 * SLOM_IQR

SLOM_outliers_count = len(houses[(houses['scaled_lot_measure15'] < SLOM_lower_bound) | (houses['scaled_lot_measure15'] > SLOM_upper_bound)])

print("Count of outliers in 'scaled_lot_measure15' column:", SLOM_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['scaled_lot_measure15'] = np.where(houses['scaled_lot_measure15'] < SLOM_lower_bound, SLOM_lower_bound, houses['scaled_lot_measure15'])
houses['scaled_lot_measure15'] = np.where(houses['scaled_lot_measure15'] > SLOM_upper_bound, SLOM_upper_bound, houses['scaled_lot_measure15'])

DSQ1 = houses['dist_station'].quantile(0.25)
DSQ3 = houses['dist_station'].quantile(0.75)
DS_IQR = DSQ3 - DSQ1

DS_lower_bound = DSQ1 - 1.5 * DS_IQR
DS_upper_bound = DSQ3 + 1.5 * DS_IQR

DS_outliers_count = len(houses[(houses['dist_station'] < DS_lower_bound) | (houses['dist_station'] > DS_upper_bound)])

print("Count of outliers in 'dist_station' column:", DS_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['dist_station'] = np.where(houses['dist_station'] < DS_lower_bound, DS_lower_bound, houses['dist_station'])
houses['dist_station'] = np.where(houses['dist_station'] > DS_upper_bound, DS_upper_bound, houses['dist_station'])

DAQ1 = houses['dist_airport'].quantile(0.25)
DAQ3 = houses['dist_airport'].quantile(0.75)
DA_IQR = DAQ3 - DAQ1

DA_lower_bound = DAQ1 - 1.5 * DA_IQR
DA_upper_bound = DAQ3 + 1.5 * DA_IQR

DA_outliers_count = len(houses[(houses['dist_airport'] < DA_lower_bound) | (houses['dist_airport'] > DA_upper_bound)])

print("Count of outliers in 'dist_airport' column:", DA_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['dist_airport'] = np.where(houses['dist_airport'] < DA_lower_bound, DA_lower_bound, houses['dist_airport'])
houses['dist_airport'] = np.where(houses['dist_airport'] > DA_upper_bound, DA_upper_bound, houses['dist_airport'])

DMQ1 = houses['dist_mopop'].quantile(0.25)
DMQ3 = houses['dist_mopop'].quantile(0.75)
DM_IQR = DMQ3 - DMQ1

DM_lower_bound = DMQ1 - 1.5 * DM_IQR
DM_upper_bound = DMQ3 + 1.5 * DM_IQR

DM_outliers_count = len(houses[(houses['dist_mopop'] < DM_lower_bound) | (houses['dist_mopop'] > DM_upper_bound)])

print("Count of outliers in 'dist_mopop' column:", DM_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['dist_mopop'] = np.where(houses['dist_mopop'] < DM_lower_bound, DM_lower_bound, houses['dist_mopop'])
houses['dist_mopop'] = np.where(houses['dist_mopop'] > DM_upper_bound, DM_upper_bound, houses['dist_mopop'])

DFQ1 = houses['dist_ferry'].quantile(0.25)
DFQ3 = houses['dist_ferry'].quantile(0.75)
DF_IQR = DFQ3 - DFQ1

DF_lower_bound = DFQ1 - 1.5 * DF_IQR
DF_upper_bound = DFQ3 + 1.5 * DF_IQR

DF_outliers_count = len(houses[(houses['dist_ferry'] < DF_lower_bound) | (houses['dist_ferry'] > DF_upper_bound)])

print("Count of outliers in 'dist_ferry' column:", DF_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['dist_ferry'] = np.where(houses['dist_ferry'] < DF_lower_bound, DF_lower_bound, houses['dist_ferry'])
houses['dist_ferry'] = np.where(houses['dist_ferry'] > DF_upper_bound, DF_upper_bound, houses['dist_ferry'])

DJQ1 = houses['dist_jgarden'].quantile(0.25)
DJQ3 = houses['dist_jgarden'].quantile(0.75)
DJ_IQR = DJQ3 - DJQ1

DJ_lower_bound = DJQ1 - 1.5 * DJ_IQR
DJ_upper_bound = DJQ3 + 1.5 * DJ_IQR

DJ_outliers_count = len(houses[(houses['dist_jgarden'] < DJ_lower_bound) | (houses['dist_jgarden'] > DJ_upper_bound)])

print("Count of outliers in 'dist_jgarden' column:", DJ_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['dist_jgarden'] = np.where(houses['dist_jgarden'] < DJ_lower_bound, DJ_lower_bound, houses['dist_jgarden'])
houses['dist_jgarden'] = np.where(houses['dist_jgarden'] > DJ_upper_bound, DJ_upper_bound, houses['dist_jgarden'])

DTQ1 = houses['dist_tmp'].quantile(0.25)
DTQ3 = houses['dist_tmp'].quantile(0.75)
DT_IQR = DTQ3 - DTQ1

DT_lower_bound = DTQ1 - 1.5 * DT_IQR
DT_upper_bound = DTQ3 + 1.5 * DT_IQR

DT_outliers_count = len(houses[(houses['dist_tmp'] < DT_lower_bound) | (houses['dist_tmp'] > DT_upper_bound)])

print("Count of outliers in 'dist_tmp' column:", DT_outliers_count)
# Replace outliers with the nearest non-outlying values
houses['dist_tmp'] = np.where(houses['dist_tmp'] < DT_lower_bound, DT_lower_bound, houses['dist_tmp'])
houses['dist_tmp'] = np.where(houses['dist_tmp'] > DT_upper_bound, DT_upper_bound, houses['dist_tmp'])

"""---


## **We have scaled the data and capped the outlying data, we shall now proceed to build the models.**

First of all, we will divide our data into train set, validation set and test set data in the ratio of 40:30:30.
"""

X = houses.drop(['log_price'], axis=1)
y = houses['log_price']

train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))

"""We **begin** by the Multiple Linear Regression, Ridge Regression, Bayesian Ridge Regression, Polynomial Regression, Lasso Regression and Decision Tree before we move onto **machine learning algorithms** like Random Forest Regressor, XG Boost Regressor and Cat Boost Regressor and then move on to **neural network framework** with Multilayer Perceptron (MLP).

### Multiple Linear Regression
"""

model_LR = LinearRegression()
model_LR.fit(X_train, y_train)
Y_LR_pred = model_LR.predict(X_val)
LR_rmse = np.sqrt(mean_squared_error(y_val, Y_LR_pred))
LR_r2 = r2_score(y_val, Y_LR_pred)
print("R-squared:", LR_r2)
print("Root Mean Squared Error:", LR_rmse)

"""### Ridge Regression"""

model = Ridge(alpha=1.0)
model.fit(X_train, y_train)
y_Rid_pred = model.predict(X_val)
r2 = r2_score(y_val, y_Rid_pred)
rmse = sqrt(mean_squared_error(y_val, y_Rid_pred))
print(f"R-squared: {r2}")
print(f"RMSE: {rmse}")

"""### Bayesian Ridge Regression"""

model = BayesianRidge()
model.fit(X_train, y_train)
y_Bay_pred = model.predict(X_val)
threshold = 0.5
y_Bay_binary_pred = (y_Bay_pred >= threshold).astype(int)
r2 = r2_score(y_val, y_Bay_pred)
rmse = sqrt(mean_squared_error(y_val, y_Bay_pred))
print(f"R-squared: {r2}")
print(f"RMSE: {rmse}")

"""### Polynomial Regression"""

degree = 2
poly = PolynomialFeatures(degree=degree)
X_poly = poly.fit_transform(X_train)
model = LinearRegression()
model.fit(X_poly, y_train)
y_PR_pred = model.predict(poly.transform(X_val))
r2 = r2_score(y_val, y_PR_pred)
rmse = sqrt(mean_squared_error(y_val, y_PR_pred))
print(f"R-squared: {r2}")
print(f"RMSE: {rmse}")

"""### Lasso Regression"""

model = Lasso(alpha=1.0)
model.fit(X_train, y_train)
y_Las_pred = model.predict(X_val)
r2 = r2_score(y_val, y_Las_pred)
rmse = sqrt(mean_squared_error(y_val, y_Las_pred))
print(f"R-squared: {r2}")
print(f"RMSE: {rmse}")

"""### Decision Tree"""

model = DecisionTreeRegressor()
model.fit(X_train, y_train)
y_DT_pred = model.predict(X_val)
r2 = r2_score(y_val, y_DT_pred)
rmse = sqrt(mean_squared_error(y_val, y_DT_pred))
print(f"R-squared: {r2}")
print(f"RMSE: {rmse}")

"""### Random Forest"""

model = RandomForestRegressor(n_estimators=100, random_state=0)
model.fit(X_train, y_train)
y_RFR_pred = model.predict(X_val)
r2 = r2_score(y_val, y_RFR_pred)
rmse = sqrt(mean_squared_error(y_val, y_RFR_pred))
print(f"R-squared: {r2}")
print(f"RMSE: {rmse}")

"""### Cat Boost"""

model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_CAT_pred = model.predict(X_val)
r2 = r2_score(y_val, y_CAT_pred)
rmse = sqrt(mean_squared_error(y_val, y_CAT_pred))
print(f"R-squared: {r2}")
print(f"RMSE: {rmse}")

"""### XG Boost"""

model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, objective='reg:squarederror')
model.fit(X_train, y_train)
y_XG_pred = model.predict(X_val)
r2 = r2_score(y_val, y_XG_pred)
rmse = sqrt(mean_squared_error(y_val, y_XG_pred))
print(f"R-squared: {r2}")
print(f"RMSE: {rmse}")

"""### Multilayer Perceptron (MLP)"""

model = MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=1000)
model.fit(X_train, y_train)
y_MLP_pred = model.predict(X_val)
r2 = r2_score(y_val, y_MLP_pred)
rmse = sqrt(mean_squared_error(y_val, y_MLP_pred))
print(f"R-squared: {r2}")
print(f"RMSE: {rmse}")

"""**Insights:**

---
We tested our data on **10 models** and reached at the following conclusion:

* Based on the R Square and the RMSE of all models, we plotted the values in a table using Excel and from there we chose **'CatBoost'** as the model best fit for our study as it had the **highest R square (0.846) and lowest RMSE (0.200)** out of all the 10 models that we tested on.

* Catboost Regressor is a Machine learning algorith that is based on Random Forest regressor which is better able to do the predictive analytics.

---
Now we shall proceed to optimize our model to see if we can improve it further.

---
For this we will be working with RFE, VIF, AIC, Adjusted R square and P-Value along with R square.

For VIF and P_Value, we shall run parallel linear OLS model as VIF and P_Value can not be obtained from CATBOOST Regressor.

---
We started by taking **top 15** important features using RFE. We'll be dropping the other columns.
"""

model = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, verbose=0,loss_function='RMSE')

rfe = RFE(model,n_features_to_select=15)

rfe = rfe.fit(X_train, y_train)

selected_features = X.columns[rfe.support_]

print("Top Features:")
print(selected_features)
print(rfe.ranking_)
list(zip(X_train.columns,rfe.support_,rfe.ranking_))

"""Now we'll run a simple CatBoost Regressor as it is the elected best based on R2 and RMSE."""

X = houses.drop(['log_price'], axis=1)
y = houses['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)
print("P-Values Line-wise:")
print(ols_model.pvalues)

"""Now, we shall try to build a model based on RFE supported columns to see what the output seems like."""

houses_rfe=houses.copy()
houses_x=houses_rfe[selected_features].copy()
X = houses_x[selected_features]
y = houses_rfe['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""Now we begin by dropping dist_jgarden as it has the highest p_value"""

houses_rfe1=houses_rfe.copy()
houses_x1=houses_rfe[selected_features].copy()
houses_x1.drop(['dist_jgarden'],axis=1, inplace=True)

X = houses_x1
y = houses_rfe1['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* This gave us a slight improvement in R2 but also gave us a little better VIF. Now we shall drop scaled_lot_measure15 as it has a P_Value of 0.456 which is greater than 0.05 the desired p_value
"""

houses_rfe2=houses_rfe1.copy()
houses_x2=houses_x1.copy()
houses_x2.drop(['scaled_lot_measure15'],axis=1, inplace=True)

X = houses_x2
y = houses_rfe2['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* Even though we saw a 2% dip in R2, we can see the improvement in VIF and we shall continue further by dropping column Dist_mopop as it has the highest VIF.
"""

houses_rfe3=houses_rfe2.copy()
houses_x3=houses_x2.copy()
houses_x3.drop(['dist_mopop'],axis=1, inplace=True)

X = houses_x3
y = houses_rfe3['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* Even though there is no significant impact on R2, we can see improvement in VIF. We shall continue with dropping dist_ferry as it has the highest VIF now.
"""

houses_rfe4=houses_rfe3.copy()
houses_x4=houses_x3.copy()
houses_x4.drop(['dist_ferry'],axis=1, inplace=True)

X = houses_x4
y = houses_rfe4['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* Again, we see improvement in VIF with no significant change in R2. We shall now drop Dist_station as it has highest VIF
"""

houses_rfe5=houses_rfe4.copy()
houses_x5=houses_x4.copy()
houses_x5.drop(['dist_station'],axis=1, inplace=True)

X = houses_x5
y = houses_rfe5['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* Here, we are able to see that there is a significant improvement in VIF and R2 as well. now we shall drop dist_airport as it currently as highes VIF
"""

houses_rfe6=houses_rfe5.copy()
houses_x6=houses_x5.copy()
houses_x6.drop(['dist_airport'],axis=1, inplace=True)

X = houses_x6
y = houses_rfe6['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* Here, slight decrease in R2 is visible with consistent improvement in VIF. now we shall drop furnished column as it has very high P_value of 0.881
"""

houses_rfe7=houses_rfe6.copy()
houses_x7=houses_x6.copy()
houses_x7.drop(['furnished'],axis=1, inplace=True)

X = houses_x7
y = houses_rfe7['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* Here, we can see that VIF and R2 has both improved. Now we drop column age at sale for a very high VIF value of 3.951345e+03.
"""

houses_rfe8=houses_rfe7.copy()
houses_x8=houses_x7.copy()
houses_x8.drop(['age_at_sale'],axis=1, inplace=True)

X = houses_x8
y = houses_rfe8['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* Here, we can see that VIF improved very slightly and R2 fell slightly. Now we drop column scaled_ceil_measure for a very high VIF value of 4.161219.
"""

houses_rfe9=houses_rfe8.copy()
houses_x9=houses_x8.copy()
houses_x9.drop(['scaled_ceil_measure'],axis=1, inplace=True)

X = houses_x9
y = houses_rfe9['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* Here, we can see that VIF has a significant improvement and R2 has a significant decrease. Now we shall drop column quality for high VIF value of 2.648680.
"""

houses_rfe10=houses_rfe9.copy()
houses_x10=houses_x9.copy()
houses_x10.drop(['quality'],axis=1, inplace=True)

X = houses_x10
y = houses_rfe10['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")
#transforming using OLS to get better picture at VIF and P_Value
X_with_const = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_with_const).fit()
print(ols_model.summary())
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]
print("VIF Values:")
print(vif_data)

"""**Insights:**
* Here we arrive at a model with a R Square of 0.7425 and VIF of 2.79, but the AIC is very high compared to other models.
* We shall take the previous model, i.e., the model before dropping the 'quality' column as that model has a high R Sqaure of 0.79, a VIF of 0.35 and a low AIC of -2190.06.
* Our final model shall be built on 6 predictors using 'yr_built', 'room_bath', 'dist_tmp','scaled_living_measure15','scaled_basement','room_bed'and'quality'.
"""

houses_residual=houses[['log_price','room_bath','yr_built','dist_tmp','scaled_living_measure15','scaled_basement','room_bed','quality']].copy()

houses_residual.columns

"""**Now we try to hypertune our model with grid search to see if we can improve our model performance.**"""

X = houses_residual.drop(['log_price'], axis=1)
y = houses_residual['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))

catboost = CatBoostRegressor()

# Define the hyperparameters grid with increased choices
param_grid = {
    'depth': [6, 8, 10, 12],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'iterations': [200, 300, 500,1000],
    'random_strength': [0.5, 1, 1.5],
    'bagging_temperature': [0.5, 1, 1.5]
}

# Perform Grid Search
grid_search = GridSearchCV(catboost, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train the model with the best hyperparameters
best_catboost = CatBoostRegressor(**best_params)
best_catboost.fit(X_train, y_train)

# Make predictions on the validation set
y_pred = best_catboost.predict(X_val)

# Evaluate the model
mse = mean_squared_error(y_val, y_pred)
print(f'Mean Squared Error on Validation Set: {mse}')

"""We tried running the hyperparameter testing for hours with no avail. Hence we decided to take the hyperparameters that were there from the beginning."""

X = houses_residual.drop(['log_price'], axis=1)
y = houses_residual['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_val - y_val_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model.rsquared)
overall_p_value = model.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")

"""**Residual Analysis**"""

X = houses_residual.drop(['log_price'], axis=1)
y = houses_residual['log_price']
train_ratio = 0.4
validation_ratio = 0.3
test_ratio = 0.3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=1 - train_ratio)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_ratio / (test_ratio + validation_ratio))
model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE', verbose=0)
model.fit(X_train, y_train)
y_val_pred = model.predict(X_val)
fig = plt.figure()
sns.distplot((y_val - y_val_pred), bins = 20)
fig.suptitle('Error Terms Analysis', fontsize = 20)
plt.xlabel('Errors', fontsize = 18)

"""**Insights:**
* As we can see in the diagram, our errors follow a normal distribution.
* This indicates that the model on an average performs accurate predictions.
* The variance of the errors is roughly constant across different levels of the predictor variables

**Predicting on Test set to check our accuracy.**
"""

y_test_pred = model.predict(X_test)

r2 = r2_score(y_test, y_test_pred)
n = len(y)
k = X.shape[1]
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)
res = y_test - y_test_pred
mse = np.mean(res**2)
llf = -0.5 * n * (1 + np.log(2 * np.pi) + np.log(mse))
aic = 2 * k - 2 * llf
X_with_const = sm.add_constant(X_train)
model_sm = sm.OLS(y_train, X_with_const).fit()
overall_vif = 1 / (1 - model_sm.rsquared)
overall_p_value = model_sm.f_pvalue
print(f"R-squared: {r2}")
print(f"Adjusted R-squared: {adjusted_r2}")
print(f"AIC: {aic}")
print(f"Overall VIF: {overall_vif}")
print(f"Overall P-Value: {overall_p_value}")

"""**Insights:**
* We can see that the accuracy of the model on Validation set is 79.34%. The accuracy of the Train set was 79.85%.
* The accuracy of the Test set is similar to that of the Validation set, meaning
our model is performing very well and there seems to be no over-fitting.

Now we'll do Model Evaluation with the actual and the predicted values by plotting a scatter plot. This will help us understand the spread much better.
"""

# Plotting y_test and y_pred to understand the spread.
fig = plt.figure()
plt.scatter(y_test,y_test_pred)
fig.suptitle('y_test vs y_test_pred', fontsize=20)
plt.xlabel('y_test ', fontsize=18)
plt.ylabel('y_test_pred', fontsize=16)

"""**Insights:**

* It supports our model by validating that our model is accurate in making predictions.
* The scatter plot follows a positive trend which means that our model makes consistent true predictions.
* We can see that there are some outliers still present.
* Since there is minimal scatter, this indicates that our model has indentified the underlying pattern.

---

We would like to present this as our final model which has an accuracy of 79.85% on the test data. It predicts a large number of it's values accurately and will work well for the prediction of house price based on the features 'room_bath', 'yr_built', 'dist_tmp', 'scaled_living_measure15', 'quality', 'scaled_basement' and 'room_bed'.
"""